ğŸ§  AI Call Agent â€“ Error Recovery & Resilience System
ğŸ“Œ Overview

This project implements a robust error recovery and resilience system for an AI Call Agent that depends on external services such as ElevenLabs, LLM providers, and CRMs.

The goal is to ensure that the system:

Detects failures accurately

Recovers intelligently

Prevents cascading outages

Alerts humans when required

Continues operating gracefully

This project was built as part of an Incubation Engineering Assignment, with a focus on engineering maturity and production-ready design, not just correctness.

ğŸ—ï¸ Architecture Overview
Call Processor
   |
   v
Service Handler (ElevenLabs)
   |
   v
Resilience Layer
â”œâ”€â”€ Error Categorization
â”œâ”€â”€ Retry Engine
â”œâ”€â”€ Circuit Breaker
â”œâ”€â”€ Health Checks
â”œâ”€â”€ Logging & Observability
â””â”€â”€ Alerting System
Key Design Principles

Clear separation of concerns

No external retry / circuit breaker libraries

Configuration-driven behavior

Non-blocking, async operations

Easy to extend and test

â— Error Categorization

Custom error hierarchy is used to differentiate failures:

Transient Errors

503 Service Unavailable

Network timeouts

Temporary outages
â†’ Eligible for retry

Permanent Errors

Authentication failures

Invalid payloads

Quota exceeded
â†’ No retries

Custom Errors

BaseError

TransientError

PermanentError

ğŸ” Retry Logic (Exponential Backoff)

Retry logic is:

Applied only to transient errors

Fully configurable

Implemented without external libraries

Retry Configuration
initialDelayMs: 5000
maxRetries: 3
backoffMultiplier: 2

Retries follow exponential backoff:

5s â†’ 10s â†’ 20s
ğŸ”Œ Circuit Breaker Pattern

Each external service has its own circuit breaker.

Circuit States

CLOSED â€“ normal operation

OPEN â€“ service unhealthy, fail fast

HALF_OPEN â€“ test recovery with limited traffic

Behavior

Opens after repeated failures

Prevents unnecessary retries when service is down

Automatically recovers when service becomes healthy

ğŸ“Š Logging & Observability

Structured logging is implemented with multiple sinks.

Log Destinations

ğŸ“„ Local file (JSON logs)

ğŸ“Š Google Sheets (mocked for visibility)

Logged Fields

Timestamp

Service name

Message

Error type (Transient / Permanent)

Retry count (when applicable)

Circuit breaker state

Logs are non-blocking and structured for easy debugging.

ğŸš¨ Alerting System

Alerts notify humans when critical failures occur.

Alert Channels

ğŸ“§ Email

ğŸ“² Telegram

ğŸŒ Webhook

(All mocked for demonstration)

Alerts Triggered When

Circuit breaker transitions to OPEN

Permanent failures occur

Dependency remains unavailable

Alerts are fired only on state transitions to avoid alert fatigue.

ğŸ©º Health Checks & Automatic Recovery

Health checks run periodically in the background, even when the circuit is OPEN.

Recovery Flow

Circuit opens due to sustained failure

Health checks continue running

On successful health check:

Circuit moves to HALF_OPEN

A test request is allowed:

Success â†’ circuit CLOSED

Failure â†’ circuit OPEN

This enables automatic recovery without manual intervention.

ğŸ¯ Required Scenario Handling (Verified)

Scenario: ElevenLabs returns 503 Service Unavailable

âœ” Detected as a transient error
âœ” Retried with exponential backoff
âœ” Retries limited to 3 attempts
âœ” Circuit breaker opens after failures
âœ” Alerts triggered for OPEN state
âœ” Calls fail fast while OPEN
âœ” Health checks recover the service
âœ” Call processing resumes automatically

ğŸ“‚ Project Structure
src/
â”œâ”€â”€ alerts/          # Email, Telegram, Webhook alerts
â”œâ”€â”€ config/          # Retry configuration
â”œâ”€â”€ errors/          # Custom error hierarchy
â”œâ”€â”€ logging/         # File + Google Sheets logging
â”œâ”€â”€ resilience/      # Retry, circuit breaker, health checks
â”œâ”€â”€ services/        # ElevenLabs client & handler
â”œâ”€â”€ queue/           # (Optional) Call queue logic
â””â”€â”€ index.ts         # Application entry point
â–¶ï¸ Running the Project
Install dependencies
npm install
Run the simulation
npx ts-node src/index.ts

The console output will demonstrate:

Retries

Circuit breaker opening

Alerts firing

Health check recovery

Automatic resumption of calls

ğŸ§ª Testing Strategy

External services are mocked to simulate:

Random failures

Service recovery

Intermittent availability

This allows deterministic testing of resilience behavior.

ğŸ§  Key Engineering Learnings

Retries alone are dangerous without circuit breakers

Fail-fast systems recover faster

Alerting should be meaningful, not noisy

Health checks are essential for self-healing systems

Clean abstractions improve reliability and maintainability

ğŸš€ Future Improvements

Real Google Sheets integration

Request correlation IDs

Graceful call queue degradation

Metrics dashboard (Prometheus / Grafana)

Unit tests for resilience components

ğŸ‘¤ Author

Anirudh Madas
Backend / Full-Stack Developer